# ai_responses.py
import logging
import random
from openai import OpenAI
import google.generativeai as genai
from config import OPENAI_API_KEY, GEMINI_API_KEY, RADIO_JAVAN_ACCESS_KEY
import requests

logger = logging.getLogger(__name__)

# Configure OpenAI with new client
openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# Configure Gemini
gemini_model = None
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        gemini_model = genai.GenerativeModel('gemini-pro')
    except Exception as e:
        logger.warning(f"Failed to configure Gemini: {e}")
        gemini_model = None

async def get_ai_response(user_message: str, user_id: int) -> str:
    """Generates a general AI response using OpenAI or Gemini, personalized for Behnoush."""
    try:
        # Use OpenAI for general conversation
        if openai_client:
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ุชู ฺฉ ุฏุณุชุงุฑ ููุฑุจุงู ู ุฏูุณุชุงูู ูุณุช ฺฉู ุจุง ูุญู ุตูู ู ุจู ูุงุฑุณ ูพุงุณุฎ ูโุฏู. ุงฺฏุฑ ูุงู ฺฉุงุฑุจุฑ ุจูููุด ุงุณุชุ ุจุง 'ุจูููุด ุฌุงู' ุฎุทุงุจุด ฺฉู."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=100,
                temperature=0.7
            )
            ai_text = response.choices[0].message.content.strip()
            return ai_text
        
        # Fallback to Gemini if OpenAI is not available
        elif gemini_model:
            try:
                response = gemini_model.generate_content(
                    f"ฺฉุงุฑุจุฑ: {user_message}\nุฑุจุงุช (ุจุง ูุญู ููุฑุจุงู ู ุฏูุณุชุงููุ ุจู ูุงุฑุณ ู ุจุง ุฎุทุงุจ 'ุจูููุด ุฌุงู' ุงฺฏุฑ ูุงู ฺฉุงุฑุจุฑ ุจูููุด ุงุณุช):"
                )
                return response.text.strip()
            except Exception as e:
                logger.warning(f"Gemini API error: {e}")
                return "ุจูููุด ุฌุงูุ ูุชุงุณูุงูู ุงูุงู ููโุชููู ุจูุช ูพุงุณุฎ ุจุฏู. ู ูุดฺฉู ฺฉูฺฺฉ ูพุด ุงููุฏู. ๐"
        
        else:
            return "ุจูููุด ุฌุงูุ ูุชุงุณูุงูู ุงูุงู ููโุชููู ุจูุช ูพุงุณุฎ ุจุฏู. ู ูุดฺฉู ฺฉูฺฺฉ ูพุด ุงููุฏู. ๐"

    except Exception as e:
        logger.error(f"AI Response Error: {e}")
        return "ุจูููุด ุฌุงูุ ูุชุงุณูุงูู ุงูุงู ููโุชููู ุจูุช ูพุงุณุฎ ุจุฏู. ู ูุดฺฉู ฺฉูฺฺฉ ูพุด ุงููุฏู. ๐"

async def search_music(query: str) -> str:
    """Search for music using Radio Javan API."""
    API_URL = "https://api.ineo-team.ir/rj.php"
    params = {
        'accessKey': RADIO_JAVAN_ACCESS_KEY,
        'action': 'search',
        'query': query
    }
    
    try:
        response = requests.post(API_URL, data=params, timeout=10)
        response.raise_for_status()
        result = response.json()
        
        if result.get('status_code') == 200:
            music_results = result.get('result', [])
            if music_results:
                response_text = "๐ต ูุชุงุฌ ุฌุณุชุฌู:\n"
                for i, music in enumerate(music_results[:3]):  # Limit to 3 results
                    title = music.get('title', 'ูุงูุดุฎุต')
                    artist = music.get('artist', 'ูุงูุดุฎุต')
                    response_text += f"{i + 1}. {title} - {artist}\n"
                return response_text
            else:
                return "ุจูููุด ุฌุงูุ ูุชุงุณูุงูู ูฺ ูุชุฌูโุง ูพุฏุง ูุดุฏ. ๐ต"
        else:
            return "ุจูููุด ุฌุงูุ ูุชุงุณูุงูู ุฏุฑ ุฌุณุชุฌู ูุดฺฉู ูพุด ุขูุฏู ุงุณุช."

    except requests.exceptions.Timeout:
        logger.error("Radio Javan API timeout")
        return "ุจูููุด ุฌุงูุ ุฌุณุชุฌู ุฎู ุทูู ฺฉุดุฏ. ูุทูุง ุฏูุจุงุฑู ุงูุชุญุงู ฺฉู."
    except requests.exceptions.RequestException as e:
        logger.error(f"Radio Javan API Error: {e}")
        return "ุจูููุด ุฌุงูุ ูุชุงุณูุงูู ุฏุฑ ุญุงู ุญุงุถุฑ ููโุชููู ุจูุช ฺฉูฺฉ ฺฉูู. ูุทูุง ุจุนุฏุง ุงูุชุญุงู ฺฉู."
    except Exception as e:
        logger.error(f"Unexpected error in music search: {e}")
        return "ุจูููุด ุฌุงูุ ู ุฎุทุง ุบุฑููุชุธุฑู ุฑุฎ ุฏุงุฏู. ูุทูุง ุฏูุจุงุฑู ุชูุงุด ฺฉู."

async def get_joke() -> str:
    """Generates a joke using OpenAI or a predefined list."""
    jokes = [
        "ฺุฑุง ฺฉุงููพูุชุฑ ุขูุณุชู ฺฉุงุฑ ูโฺฉุฑุฏุ ฺูู ุฑู ฺฉุงููพูุชุฑ ุฎูุงุจุด ูููุฏ! ๐ป๐ด",
        "ุขูุง ู ุชูุณุงุญ ฺุทูุฑ ุชุงุจุณุชูู ุฑู ูโฺฏุฐุฑูููุ ุจุง ฺฉููุฑ ฺฉุฑูฺฉูุฏู! ๐โ๏ธ",
        "ูโุฏูู ฺุฑุง ุฏุงูุดุฌููุง ุณุงูุฏูฺ ุฏูุณุช ุฏุงุฑูุฏุ ฺูู ู ุฑูุฒ ู ุงุณุชุงุฏ ฺฏูุช: ูุฑ ฺฉ ุชฺฉููุด ุฑู ูุฏู ุณุงูุฏูฺ ูุดู! ๐ฅช๐",
        "ุจูููุด ุฌุงูุ ูโุฏูู ฺุฑุง ูฺฉโูฺฉโูุง ูููุน ุฎูุงุจ ู ูพุงุดูู ุฑู ุจุงูุง ูฺฏู ูโุฏุงุฑูุ ฺูู ุงฺฏู ูุฑ ุฏู ูพุงุดูู ุฑู ุจุฐุงุฑู ุฒููุ ูโุงูุชู! ๐๐ฆฉ"
    ]
    
    try:
        if openai_client:
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ฺฉ ุฌูฺฉ ฺฉูุชุงู ู ุจุงูุฒู ุจู ูุงุฑุณ ุจฺฏู"},
                    {"role": "user", "content": "ู ุฌูฺฉ ุจฺฏู"}
                ],
                max_tokens=50,
                temperature=0.8
            )
            joke = response.choices[0].message.content.strip()
            return f"ุจูููุด ุฌุงูุ {joke} ๐"
        else:
            return random.choice(jokes)
    except Exception as e:
        logger.error(f"OpenAI Joke Error: {e}")
        return random.choice(jokes)

async def get_supportive_message() -> str:
    """Generates a supportive message using Gemini or a predefined list."""
    supportive_messages = [
        "ุชู ุฎู ููโุชุฑ ุงุฒ ุงู ุญุฑูุง! ูโุฏููู ูโุชูู ุงุฒ ูพุณ ูุฑ ฺุฒ ุจุฑุจุง. ๐ช",
        "ุจุฏูู ฺฉู ูุฑ ุฑูุฒ ู ูุฏู ุจู ุจูุชุฑ ุดุฏู ูุฒุฏฺฉโุชุฑ ูุดุ ูู ููุดู ุงูุฌุง ูุณุชู ุจุฑุงุช. ๐",
        "ุฒูุฏฺฏ ูุซู ู ุฌุนุจู ุดฺฉูุงุชูุ ุดุงุฏ ุจุนุถ ููุงูุน ูุฒูโุงุด ุฑู ูููู ุงูุง ูููุฒ ุดฺฉูุงุชโูุง ุฎูุดูุฒู ุฒุงุฏ ูููุฏู! ๐ซ",
        "ุงุฏุช ุจุงุดู ุจูููุด ุฌุงูุ ุจุนุฏ ุงุฒ ูุฑ ุณุฎุชุ ุขุณุงู ูุณุช. ูู ุจูุช ุงูุงู ุฏุงุฑู! ๐",
        "ุจูููุด ุนุฒุฒูุ ูุจุฎูุฏ ุชู ูุดูฺฏโุชุฑู ฺุฒู ฺฉู ูโุชููู ุจุจูู. ุงูุฏูุงุฑู ุฒูุฏุชุฑ ุญุงูุช ุฎูุจ ุจุดู. ๐"
    ]
    
    try:
        if gemini_model:
            response = gemini_model.generate_content(
                "ฺฉ ูพุงู ฺฉูุชุงูุ ูุซุจุช ู ุงูุฏุจุฎุด ุจู ูุงุฑุณ ุจุฑุง ฺฉุณ ฺฉู ฺฉู ูุงุฑุงุญุช ุงุณุชุ ุจุง ุฎุทุงุจ 'ุจูููุด ุฌุงู' ุจููุณ:",
                generation_config=genai.types.GenerationConfig(
                    temperature=0.9,
                    max_output_tokens=80,
                )
            )
            message = response.text.strip()
            return message
        else:
            return random.choice(supportive_messages)
    except Exception as e:
        logger.error(f"Gemini Supportive Message Error: {e}")
        return random.choice(supportive_messages)
